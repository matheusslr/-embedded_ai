## PT-BR version
A implantação de aplicativos de aprendizado de máquina em dispositivos de borda pode trazer benefícios claros, como confiabilidade, latência e privacidade aprimoradas, mas também apresenta seu próprio conjunto de desafios. A maioria dos trabalhos se concentra nos recursos computacionais limitados das plataformas de borda, mas esse não é o único gargalo no caminho da adoção generalizada. O presente artigo relata uma visão geral das dificuldades de operação quando um desenvolvedor opta por uma implantação baseada em borda em sua aplicação ML. Além disso, é apresentado uma comparação da implementações de aplicações de borda com as implementações baseadas em nuvem.

TinyML, uma vertente emergente que implementa algoritmos de Machine Learning (ML) em sistemas de baixo consumo de energia, é um paradigma de implantação atraente para várias aplicações, como eletrodomésticos inteligentes e assistentes virtuais. Para esses aplicativos, a implantação de borda pode fornecer uma latência menor, maior robustez, escalabilidade e privacidade em comparação com cenários de implantação em que o modelo é avaliado em infraestrutura em nuvem. Entretanto, existem vários desafios que dificultam a adoção em larga escala da IA de ponta. O mais óbvio é o poder de processamento limitado dos dispositivos de borda, uma vez que dispositivos são normalmente vestíveis e alimentados por bateria, restringido ao consumo de energia. Além disso, fator de forma e dissipação de calor também limitam seus recursos computacionais. A maior parte do trabalho no campo do TinyML se concentra em melhorar a eficiência dos modelos em dispositivos com recursos limitados. Neste artigo, foca-se menos no aspecto computacional, mas investigou-se quais outros desafios surgem quando o e ML de borda é colocado em produção e defende a extensão de MLOps para TinyMLOps como solução.

Em um aplicativo centralizado baseado em nuvem, provavelmente é suficiente ter um único modelo para todos os usuários. Em contrapartida, em aplicativos descentralizados, diferentes usuários terão diferentes dispositivos com diferentes recursos computacionais, disponibilidade de armazenamento e conectividade de rede. As consequencias disso é diversas: em vez de treinar um único modelo, será necessario dar suporte a vários modelos, cada um com seu próprio custo computacional e compensação de precisão; O melhor modelo para um determinado dispositivo pode depender de fatores externos além dos recursos computacionais do dispositivo. Se o dispositivo estiver conectado a uma fonte de alimentação externa, o consumo de energia pode ser um problema menor do que quando está desconectado e depende da energia da bateria. Ou seja, como diferentes plataformas de hardware podem suportar um conjunto diferente de operações, talvez seja necessário desenvolver diferentes versões de um modelo, cada uma direcionada a uma determinada plataforma. Esses exemplos mostram que o número de modelos que precisam ser gerenciados por um sistema TinyMLOps é muito maior do que o número de modelos para uma implantação centralizada correspondente.

Os modelos de ML estão sendo cada vez mais usados em grandes aplicativos voltados para o consumidor. Sendo assim, o monitoramento e a observabilidade são fundamentais para garantir que um modelo continue funcionando conforme o esperado. É relativamente simples implementar a observabilidade do modelo quando o modelo é implantado em uma plataforma de nuvem, pois todos os dados de entrada são enviados para um local centralizado. Contudo, em modelos implantados na borda, torna-se menos trivial. Um grande benefício da implantação de borda é que nenhum dado precisa sair do dispositivo, o que oferece garantias de privacidade mais fortes do que o processamento em nuvem. Mas esse argumento seria invalidado se compartilhássemos dados periodicamente com a nuvem para fins de análise e reciclagem.
Outra adversidade explorada no artigo são os modelos de negócios de pagamento por consulta. A aplicação Cloud Vision do Google, por exemplo, cobra dos usuários US$ 1,50 por 1.000 solicitações para tarefas como detecção de rosto. Semelhante à observabilidade, é trivial implementar isso em uma API na nuvem, pois todas as solicitações precisam ser processadas pelo mesmo endpoint de maneira online. No entanto, em aplicações de borda, seria muito mais difícil de implementar, pois o modelo agora é replicado em um grande número de dispositivos do usuário final que podem nem estar conectados à Internet no momento em que estão avaliando o modelo. 

Por fim, os aplicativos modernos de aprendizado de máquina não são mais estáticos, eles são atualizados continuamente à medida que novos dados são observados. Isso permite que eles lidem com mudanças nos dados de entrada e beneficiam o desempenho geral do sistema. Existem alguns desafios, como lidar com o esquecimento catastrófico (redes neurais treinadas em novas tarefas que esquece, o que aprenderam com tarefas anteriores). Em um cenário de nuvem, todos os dados são transmitidos para a nuvem para processamento. Embora seja trabalhoso, é relativamente fácil armazenar esses dados, limpá-los e potencialmente anotá-los para treinar novas iterações do modelo. Depois que um modelo atualizado é treinado, podemos validá-lo em um conjunto de dados de retenção e substituir o modelo existente se sua precisão for satisfatória. Em um aplicativo baseado em borda, isso está longe de ser trivial. Para isso, teremos que contar com o Federated Learning para atualizar o modelo. Com o Federated Learning, um usuário baixa o modelo atual e o atualiza localmente com seus próprios dados. Pórem, o Federated Learning não é um problema trivial, pois os dados heterogêneos (não-iid) gerados por diferentes clientes fazem com que os modelos locais divirjam, dificultando a agregação das atualizações locais em uma atualização global.


## EN version
Deploying machine learning applications on edge devices can bring clear benefits such as improved reliability, latency and privacy, but it also presents its own set of challenges. Most of the work focuses on the limited computing resources of edge platforms, but this is not the only bottleneck in the way of widespread adoption. This article reports an overview of the operational difficulties when a developer opts for an edge-based deployment in their ML application. In addition, a comparison of implementations of edge applications with cloud-based implementations is presented.

TinyML, an emerging strand that implements Machine Learning (ML) algorithms in energy-efficient systems, is an attractive deployment paradigm for various applications such as smart home appliances and virtual assistants. For these applications, edge deployment can provide lower latency, greater robustness, scalability, and privacy compared to deployment scenarios where the model is evaluated on cloud infrastructure. However, there are a number of challenges that make it difficult for large-scale adoption of cutting-edge AI. The most obvious is the limited processing power of edge devices, as devices are typically wearable and battery powered, constrained by power consumption. In addition, form factor and heat dissipation also limit your computing resources. Most of the work in the TinyML field focuses on improving the efficiency of models on devices with limited resources. In this article, we focus less on the computational aspect, but investigated what other challenges arise when edge ML is put into production and advocates extending MLOps to TinyMLOps as a solution.

In a centralized cloud-based application, it is probably sufficient to have a single model for all users. In contrast, in decentralized applications, different users will have different devices with different computing resources, storage availability and network connectivity. The consequences of this are diverse: instead of training a single model, it will be necessary to support several models, each with its own computational cost and precision compensation; The best model for a given device may depend on external factors beyond the device's computing resources. If the device is connected to an external power source, power consumption may be less of an issue than when it is unplugged and relies on battery power. That is, because different hardware platforms may support a different set of operations, it may be necessary to develop different versions of a model, each targeting a particular platform. These examples show that the number of models that need to be managed by a TinyMLOps system is much greater than the number of models for a corresponding centralized deployment.

ML models are increasingly being used in large consumer-facing applications. As such, monitoring and observability are critical to ensuring that a model continues to perform as expected. It is relatively simple to implement model observability when the model is deployed on a cloud platform, as all input data is sent to a centralized location. However, in models deployed at the edge, it becomes less trivial. A major benefit of edge deployment is that no data has to leave the device, which offers stronger privacy guarantees than cloud processing. But this argument would be invalidated if we periodically share data with the cloud for analysis and recycling purposes.

Another adversity explored in the article is the pay-per-query business models. Google's Cloud Vision application, for example, charges users $1.50 per 1,000 requests for tasks like face detection. Similar to observability, it is trivial to implement this in a cloud API as all requests need to be processed by the same endpoint online. However, in edge applications it would be much more difficult to implement as the model is now replicated across a large number of end-user devices that may not even be connected to the internet at the time they are evaluating the model.

Finally, modern machine learning applications are no longer static, they are continually updated as new data is observed. This allows them to handle changes in input data and benefit the overall performance of the system. There are some challenges, such as dealing with catastrophic forgetting (neural networks trained on new tasks that you forget, what they learned from previous tasks). In a cloud scenario, all data is streamed to the cloud for processing. While it is labor intensive, it is relatively easy to store this data, clean it up, and potentially annotate it to train new iterations of the model. Once an updated model is trained, we can validate it against a retention dataset and replace the existing model if its accuracy is satisfactory. In an edge-based application, this is far from trivial. For this, we will have to rely on Federated Learning to update the model. With Federated Learning, a user downloads the current model and updates it locally with their own data. However, Federated Learning is not a trivial issue, as heterogeneous (non-iid) data generated by different clients causes local models to diverge, making it difficult to aggregate local updates into a global update.
